# Speed Layer Workflow
## Step 1: Generate mock data and store it in the designated folder
- Generate mock data from `src_sys.sh` to simulate real-time transaction log files. stored in two directories: `/data/flume/source/hdfs` and `/data/flume/source/hbase`

### Create two directories within the Docker container to store mock data generated by `src_sys.sh`

```sh
$ mkdir /flume/source/hdfs
```
```sh
$ mkdir /data/flume/source/hbase
```
<img src="image\mkdir hdfs,hbase.png" width=100% height=40%>

### src_sys.sh 
```sh
#!/bin/bash

declare -a orders=("espresso 65" "cappucino 90" "mocha 80" "latte 70" "chocolate 60" "greentea 60")

for i in $(seq 1 100)
do
    while true
    do
        id=$((1 + $RANDOM % 1000))
        if ((id >= 1 && id <= 801)) || ((id >= 5001 && id <= 5945)) || ((id >= 8000 && id <= 8501));
        then
            break
        else
            continue
        fi
    done
   timestamp=$(date +%s)
   echo "$id|${orders[$RANDOM%${#orders[@]}]}|$timestamp" >> /data/flume/source/hdfs/order_${timestamp}.txt
   echo "$id|${orders[$RANDOM%${#orders[@]}]}|$timestamp" >> /data/flume/source/hbase/order_${timestamp}.txt
   sleep 30
done
```
### The command to run shell
```sh
$ nohup sh /data/flume/src_sys.sh &
```
<img src="image\command run shell.png" width=100% height=40%>

### View jobs
```sh
$ jobs -l
```
<img src="image\job src_sys.png" width=100% height=40%>

---

## Step 2.1: Ingest log files into HDFS using Apache Flume
### Create a directory in Hadoop HDFS to ingest log files via Apache Flume
```sh
$ hadoop fs -mkdir /tmp/flume/sink
```

### Configure Apache Flume to ingest log files from sources and write them to HDFS

- flume_hdfs.conf

```sh
tier1.channels = ch-1
tier1.sources = src-1
tier1.sinks = snk-1

tier1.channels.ch-1.type = memory
tier1.channels.ch-1.capacity = 1000
tier1.channels.ch-1.transactionCapacity = 100

tier1.sources.src-1.type = spooldir
tier1.sources.src-1.spoolDir = /data/flume/source/hdfs
tier1.sources.src-1.fileHeader = false
tier1.sources.src-1.fileSuffix = .COMPLETED

tier1.sinks.snk-1.type = hdfs
tier1.sinks.snk-1.hdfs.path = /tmp/flume/sink/
tier1.sinks.snk-1.hdfs.writeFormat = Text
tier1.sinks.snk-1.hdfs.fileType = DataStream

tier1.sources.src-1.channels = ch-1
tier1.sinks.snk-1.channel = ch-1
```
### The command to run an Apache Flume agent
```sh
$ nohup flume-ng agent -n tier1 -f /data/flume/source/flume_hdfs.conf &
```
<img src="image\command Flume HDFS.png" width=100% height=40%>

### View jobs
```sh
$ jobs -l
```
<img src="image\job hdfs.png" width=100% height=40%>

### Result
<img src="image\result in docker hdfs.png" width=100% height=40%>
<img src="image\result in hdfs.png" width=100% height=40%>

---

## Step 2.2: Ingest log files into HBase using Apache Flume
### Create an HBase table in Hadoop for ingesting log files using Apache Flume

```sh
$ hbase shell
```
<img src="image\hbase shell.png" width=100% height=40%>

#### List all tables in HBase
```sh
$ list
```
<img src="image\list.png" width=40% height=40%>

#### HBase in Hue
<img src="image\HBase in Hue.png" width=100% height=40%>
<img src="image\HBase UI.png" width=100% height=40%>

#### Create table in HBase
```sh
$ create 'spooled_table' 'spool_cf' 
```
<img src="image\create table.png" width=60% height=40%>
<img src="image\table in Hue UI.png" width=100% height=40%>

#### View data in table
```sh
$ scan 'spooled_table' 
```
<img src="image\scan table.png" width=60% height=40%>
<img src="image\view data in table.png" width=100% height=40%>

#### Exit from HBase
```sh
$ exit 
```
<img src="image\exit HBase.png" width=40% height=40%>

### Configure Apache Flume to ingest log files from sources and write them to HBase

-  flume_hbase.conf
```sh
tier2.channels = ch-2
tier2.sources = src-2
tier2.sinks = snk-2

tier2.channels.ch-2.type = memory
tier2.channels.ch-2.capacity = 1000
tier2.channels.ch-2.transactionCapacity = 100

tier2.sources.src-2.type = spooldir
tier2.sources.src-2.spoolDir = /data/flume/source/hbase
tier2.sources.src-2.fileHeader = false
tier2.sources.src-2.fileSuffix = .COMPLETED

tier2.sinks.snk-2.type = org.apache.flume.sink.hbase.HBaseSink
tier2.sinks.snk-2.table = spooled_table
tier2.sinks.snk-2.columnFamily = spool_cf
tier2.sinks.snk-2.serializer = org.apache.flume.sink.hbase.SimpleHbaseEventSerializer
tier2.sinks.snk-2.serializer.payloadColumn = col1

tier2.sources.src-2.channels = ch-2
tier2.sinks.snk-2.channel = ch-2
```
### The command to run an Apache Flume agent
```sh
$ nohup flume-ng agent -n tier2 -f /data/flume/source/flume_hbase.conf &
```
<img src="image\command Flume HBase.png" width=100% height=40%>

### View jobs
```sh
$ jobs -l
```
<img src="image\jobs hbase.png" width=100% height=40%>

### Result
<img src="image\result in HBase.png" width=100% height=40%>

---

## Step 3: Create a Hive table on top of SequenceFiles to convert them into structured data
<img src="image\go to Hive.png" width=100% height=40%>
<img src="image\Hive UI.png" width=100% height=40%>

### Create a Hive table named `transactions`
```sh
CREATE TABLE default.transactions(
   customer_id         int
  ,customer_order  	string
  ,order_timestamp	bigint
) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '|' 
LINES TERMINATED BY '\n' 
STORED AS INPUTFORMAT 
'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/tmp/flume/sink/'
```

<img src="image\create hive transactions.png" width=100% height=40%>

---

## Step 4: Data cleansing using Spark Streaming
### spark_streaming.py
```sh
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import Row, SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder.appName("strm_spark_job").enableHiveSupport().getOrCreate()
spark.conf.set("spark.sql.streaming.checkpointLocation", "/tmp/checkpoints")

userSchema = StructType().add("customer_id", "integer").add("customer_order", "string").add("order_timestamp", "integer")

strmDF = spark \
    .readStream \
    .schema(userSchema) \
    .option("sep", "|") \
    .csv("/tmp/flume/sink")

split_col = split(strmDF['customer_order'], ' ')
strmDF = strmDF.withColumn('order_menu', split_col.getItem(0)) \
.withColumn('order_price', split_col.getItem(1).cast('integer')) \
.withColumn("order_timestamp",from_unixtime(col("order_timestamp"),'dd-MM-yyyy HH:mm:ss').cast('string')) \
.drop('customer_order')


query = strmDF \
    .selectExpr('customer_id as cust_id','order_menu as odr_menu','order_price as odr_prc','order_timestamp as odr_tms') \
    .writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "/tmp/default/transactions_cln") \
    .start()

query.awaitTermination()
```
### Run Spark Submit
```sh
$ nohup spark-submit /data/spark_streaming/spark_streaming.py &
```
<img src="image\Run Spark Submit.png" width=100% height=40%>

### View jobs
```sh
$ jobs -l
```
<img src="image\job spark streaming.png" width=100% height=40%>

### Result
<img src="image\result spark streaming.png" width=100% height=40%>

---

## Step 5: Create a Hive table on top of cleaned data to transform it into a structured format
<img src="image\go to Hive.png" width=100% height=40%>
<img src="image\Hive UI.png" width=100% height=40%>

### Create a Hive table named `transactions_cln`
```sh
CREATE EXTERNAL TABLE transactions_cln(
cust_id int
,odr_menu string
,odr_prc int
,odr_tms string
) 
STORED AS PARQUET
LOCATION '/tmp/default/transactions_cln/'
```

<img src="image\create hive transactions_cln.png" width=100% height=40%>

---
